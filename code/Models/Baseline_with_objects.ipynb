{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from basicmemnet import memnet\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_train = memnet.DSL()\n",
    "md_test = memnet.DSL()\n",
    "\n",
    "md_train.import_gml(\"train_graph.gml\")\n",
    "md_test.import_gml(\"test_graph.gml\")\n",
    "\n",
    "test = md_test.get_graph()\n",
    "train = md_train.get_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_actions = ['task_1_k_cooking', 'task_2_k_cooking_with_bowls', 'task_3_k_pouring', 'task_4_k_wiping',\n",
    "                  'task_5_k_cereals', 'task_6_w_hard_drive', 'task_7_w_free_hard_drive', 'task_8_w_hammering', \n",
    "                  'task_9_w_sawing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stir has_object whisk\n",
      "stir has_next place\n"
     ]
    }
   ],
   "source": [
    "for a, b, link in test.out_edges('668f64fb8cc61c379809bfee', data=True):\n",
    "    print(test.nodes[a].get('utterances')[0], link['link_type'], test.nodes[b].get('utterances')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dictionary for Rule-based Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {}\n",
    "for u, v, data in train.edges(data=True):\n",
    "    relation = data.get('link_type')\n",
    "\n",
    "    if relation == 'has_element':\n",
    "        head = train.nodes[u].get('utterances')[0] # head is parent action (e.g. cooking, cereals...)\n",
    "        tail = train.nodes[v].get('utterances')[0] # tail is sub-action (e.g. approach, lift...)\n",
    "\n",
    "        # If tail action has object, keep it. \n",
    "        for a, b, link in train.out_edges(v, data=True):\n",
    "            if link['link_type'] == 'has_object':\n",
    "                tail = tail + ' ' + train.nodes[b].get('utterances')[0]\n",
    "                # As a result, tail will look like: stir whisk, cut knife, etc...\n",
    "\n",
    "        if tail not in dict.keys():\n",
    "            dict[tail] = {}\n",
    "\n",
    "        if head not in dict[tail].keys():\n",
    "            dict[tail][head] = 1\n",
    "        else:\n",
    "            dict[tail][head] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_dict = {}\n",
    "\n",
    "for action, children in dict.items():\n",
    "    # Find the child node with the maximum count\n",
    "    max_child = max(children, key=children.get)\n",
    "    baseline_dict[action] = max_child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idle': 'task_8_w_hammering',\n",
       " 'approach whisk': 'task_1_k_cooking',\n",
       " 'lift whisk': 'task_1_k_cooking',\n",
       " 'stir whisk': 'task_1_k_cooking',\n",
       " 'place whisk': 'task_1_k_cooking',\n",
       " 'retreat whisk': 'task_1_k_cooking',\n",
       " 'approach bowl': 'task_2_k_cooking_with_bowls',\n",
       " 'hold bowl': 'task_1_k_cooking',\n",
       " 'retreat bowl': 'task_2_k_cooking_with_bowls',\n",
       " 'approach bottle': 'task_3_k_pouring',\n",
       " 'lift bottle': 'task_3_k_pouring',\n",
       " 'pour bottle': 'task_3_k_pouring',\n",
       " 'hold bottle': 'task_4_k_wiping',\n",
       " 'place bottle': 'task_3_k_pouring',\n",
       " 'retreat bottle': 'task_3_k_pouring',\n",
       " 'approach cereals': 'task_5_k_cereals',\n",
       " 'lift cereals': 'task_5_k_cereals',\n",
       " 'pour cereals': 'task_5_k_cereals',\n",
       " 'place cereals': 'task_5_k_cereals',\n",
       " 'retreat cereals': 'task_5_k_cereals',\n",
       " 'lift bowl': 'task_2_k_cooking_with_bowls',\n",
       " 'pour bowl': 'task_2_k_cooking_with_bowls',\n",
       " 'place bowl': 'task_2_k_cooking_with_bowls',\n",
       " 'idle whisk': 'task_2_k_cooking_with_bowls',\n",
       " 'retreat': 'task_2_k_cooking_with_bowls',\n",
       " 'idle bowl': 'task_2_k_cooking_with_bowls',\n",
       " 'approach cup': 'task_3_k_pouring',\n",
       " 'lift cup': 'task_3_k_pouring',\n",
       " 'hold cup': 'task_3_k_pouring',\n",
       " 'drink cup': 'task_3_k_pouring',\n",
       " 'place cup': 'task_3_k_pouring',\n",
       " 'retreat cup': 'task_3_k_pouring',\n",
       " 'idle bottle': 'task_3_k_pouring',\n",
       " 'approach sponge': 'task_4_k_wiping',\n",
       " 'wipe sponge': 'task_4_k_wiping',\n",
       " 'retreat sponge': 'task_4_k_wiping',\n",
       " 'hold cereals': 'task_4_k_wiping',\n",
       " 'hold whisk': 'task_4_k_wiping',\n",
       " 'approach knife': 'task_5_k_cereals',\n",
       " 'lift knife': 'task_5_k_cereals',\n",
       " 'cut knife': 'task_5_k_cereals',\n",
       " 'pour cuttingboard': 'task_5_k_cereals',\n",
       " 'place cuttingboard': 'task_5_k_cereals',\n",
       " 'retreat cuttingboard': 'task_5_k_cereals',\n",
       " 'approach cuttingboard': 'task_5_k_cereals',\n",
       " 'hold cuttingboard': 'task_5_k_cereals',\n",
       " 'lift cuttingboard': 'task_5_k_cereals',\n",
       " 'retreat knife': 'task_5_k_cereals',\n",
       " 'pour cup': 'task_5_k_cereals',\n",
       " 'approach screwdriver': 'task_7_w_free_hard_drive',\n",
       " 'lift screwdriver': 'task_7_w_free_hard_drive',\n",
       " 'screw screwdriver': 'task_6_w_hard_drive',\n",
       " 'place screwdriver': 'task_7_w_free_hard_drive',\n",
       " 'retreat screwdriver': 'task_7_w_free_hard_drive',\n",
       " 'approach harddrive': 'task_6_w_hard_drive',\n",
       " 'hold harddrive': 'task_6_w_hard_drive',\n",
       " 'retreat harddrive': 'task_6_w_hard_drive',\n",
       " 'lift harddrive': 'task_6_w_hard_drive',\n",
       " 'place harddrive': 'task_6_w_hard_drive',\n",
       " 'idle screwdriver': 'task_6_w_hard_drive',\n",
       " 'approach': 'task_6_w_hard_drive',\n",
       " 'hold screwdriver': 'task_7_w_free_hard_drive',\n",
       " 'approach hammer': 'task_8_w_hammering',\n",
       " 'lift hammer': 'task_8_w_hammering',\n",
       " 'hammer hammer': 'task_8_w_hammering',\n",
       " 'place hammer': 'task_8_w_hammering',\n",
       " 'retreat hammer': 'task_8_w_hammering',\n",
       " 'approach woodenwedge': 'task_8_w_hammering',\n",
       " 'lift woodenwedge': 'task_8_w_hammering',\n",
       " 'place woodenwedge': 'task_8_w_hammering',\n",
       " 'hold woodenwedge': 'task_8_w_hammering',\n",
       " 'retreat woodenwedge': 'task_8_w_hammering',\n",
       " 'idle hammer': 'task_8_w_hammering',\n",
       " 'approach saw': 'task_9_w_sawing',\n",
       " 'lift saw': 'task_9_w_sawing',\n",
       " 'saw saw': 'task_9_w_sawing',\n",
       " 'place saw': 'task_9_w_sawing',\n",
       " 'retreat saw': 'task_9_w_sawing',\n",
       " 'approach banana': 'task_5_k_cereals',\n",
       " 'lift banana': 'task_4_k_wiping',\n",
       " 'hold banana': 'task_5_k_cereals',\n",
       " 'place banana': 'task_4_k_wiping',\n",
       " 'retreat banana': 'task_5_k_cereals',\n",
       " 'cut cuttingboard': 'task_5_k_cereals',\n",
       " 'idle cup': 'task_6_w_hard_drive',\n",
       " 'screw harddrive': 'task_6_w_hard_drive',\n",
       " 'hold': 'task_8_w_hammering',\n",
       " 'hold saw': 'task_9_w_sawing',\n",
       " 'idle woodenwedge': 'task_9_w_sawing',\n",
       " 'stir bowl': 'task_2_k_cooking_with_bowls',\n",
       " 'place knife': 'task_5_k_cereals',\n",
       " 'pour': 'task_5_k_cereals',\n",
       " 'lift': 'task_6_w_hard_drive',\n",
       " 'place': 'task_6_w_hard_drive',\n",
       " 'hold hammer': 'task_8_w_hammering',\n",
       " 'screw': 'task_6_w_hard_drive',\n",
       " 'hold knife': 'task_5_k_cereals'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Predictions by Baseline Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_utterances = []  # True values\n",
    "predictions = []  # Top 1 predictions\n",
    "predictions_probab = []  # All predictions - so this one will be the list of dictionaries, \n",
    "                         # with parent actions as keys, number of votes for that action as values.\n",
    "\n",
    "for node in test.nodes(data=True):\n",
    "    node_id = node[0]   # String, e.g. \"658190c06eccd77ab5dc84d4\"\n",
    "    node_data = node[1] # Dictionary with type, utterances, timestamp, etc.\n",
    "\n",
    "    if node_data['utterances'][0] in parent_actions: # If the node is a parent action node\n",
    "\n",
    "        real_utterance = node_data['utterances'][0] \n",
    "        predicted_utterances = [] \n",
    "\n",
    "        # Iterate through all the sub-actions and make predictions using baseline_dict.\n",
    "        for u, v, data in test.edges(node_id, data=True):\n",
    "            if data.get('link_type') == 'has_element':\n",
    "                sub_action = test.nodes[v].get('utterances')[0]\n",
    "                \n",
    "                # If sub action has object, keep it. \n",
    "                for a, b, link in test.out_edges(v, data=True):\n",
    "                    if link['link_type'] == 'has_object':\n",
    "                        sub_action = sub_action + ' ' + test.nodes[b].get('utterances')[0]\n",
    "                        # As a result, sub action will look like: stir whisk, cut knife, etc...\n",
    "\n",
    "                prediction = baseline_dict.get(sub_action)\n",
    "                predicted_utterances.append(prediction)\n",
    "                    \n",
    "        # Keep top 1 prediction\n",
    "        predicted_utterance = max(predicted_utterances,key=predicted_utterances.count) \n",
    "        predictions.append(predicted_utterance)\n",
    "        # Keep the real value\n",
    "        real_utterances.append(real_utterance)\n",
    "        # Keep all the predictions with corresponding votes\n",
    "        predictions_probab.append({i:predicted_utterances.count(i) for i in set(predicted_utterances)})\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_at_one(true, pred):\n",
    "    # true and pred are lists\n",
    "\n",
    "    n = len(true)\n",
    "    tp = sum([int(true[i]==pred[i]) for i in range (n)])\n",
    "    return tp/n\n",
    "\n",
    "def hits_at_k(true, pred, k):\n",
    "    # true is a list, pred is a list of dictionaries, \n",
    "    # e.g. {'task_5_k_cereals': 8, 'task_8_w_hammering': 4, 'task_9_w_sawing': 1}\n",
    "\n",
    "    n = len(true)\n",
    "    hits = np.zeros(n)\n",
    "    \n",
    "    for p in range (1, k+1):\n",
    "        # get key with p-th larest value from each dictionary in the pred list\n",
    "        pred_i = [sorted(pred[i], key=pred[i].get)[-p] if p <= len(pred[i]) else 'None' for i in range (n)]\n",
    "        for hit in range (n):\n",
    "            if hits[hit] == 0 and pred_i[hit] == true[hit]:\n",
    "                hits[hit] = 1\n",
    "        \n",
    "    tp = sum(hits)\n",
    "    return tp/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits_at_one(real_utterances, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits@1:  0.7592592592592593\n",
      "Hits@2:  0.9722222222222222\n",
      "Hits@3:  1.0\n"
     ]
    }
   ],
   "source": [
    "for k in range (1, 4):\n",
    "    print(f'Hits@{k}: ', hits_at_k(real_utterances, predictions_probab, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 3,  9,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, 12,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  9,  3,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0, 12,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  9,  3,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  3,  9,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 12,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  9,  3]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(real_utterances, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
