{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from basicmemnet import memnet\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_train = memnet.DSL()\n",
    "md_test = memnet.DSL()\n",
    "\n",
    "md_train.import_gml(\"train_graph.gml\")\n",
    "md_test.import_gml(\"test_graph.gml\")\n",
    "\n",
    "test = md_test.get_graph()\n",
    "train = md_train.get_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_actions = ['task_1_k_cooking', 'task_2_k_cooking_with_bowls', 'task_3_k_pouring', 'task_4_k_wiping',\n",
    "                  'task_5_k_cereals', 'task_6_w_hard_drive', 'task_7_w_free_hard_drive', 'task_8_w_hammering', \n",
    "                  'task_9_w_sawing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dictionary for Rule-based Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {}\n",
    "for u, v, data in train.edges(data=True):\n",
    "    relation = data.get('link_type')\n",
    "\n",
    "    if relation == 'has_element':\n",
    "        head = train.nodes[u].get('utterances')[0] # head is parent action (e.g. cooking, cereals...)\n",
    "        tail = train.nodes[v].get('utterances')[0] # tail is sub-action (e.g. approach, lift...)\n",
    "\n",
    "        if tail not in dict.keys():\n",
    "            dict[tail] = {}\n",
    "\n",
    "        if head not in dict[tail].keys():\n",
    "            dict[tail][head] = 1\n",
    "        else:\n",
    "            dict[tail][head] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idle': {'task_1_k_cooking': 96, 'task_2_k_cooking_with_bowls': 96, 'task_3_k_pouring': 96, 'task_4_k_wiping': 94, 'task_5_k_cereals': 115, 'task_6_w_hard_drive': 111, 'task_7_w_free_hard_drive': 100, 'task_8_w_hammering': 163, 'task_9_w_sawing': 96}, 'approach': {'task_1_k_cooking': 192, 'task_2_k_cooking_with_bowls': 192, 'task_3_k_pouring': 106, 'task_4_k_wiping': 156, 'task_5_k_cereals': 382, 'task_6_w_hard_drive': 339, 'task_7_w_free_hard_drive': 233, 'task_8_w_hammering': 303, 'task_9_w_sawing': 129}, 'lift': {'task_1_k_cooking': 96, 'task_2_k_cooking_with_bowls': 96, 'task_3_k_pouring': 129, 'task_4_k_wiping': 100, 'task_5_k_cereals': 226, 'task_6_w_hard_drive': 140, 'task_7_w_free_hard_drive': 135, 'task_8_w_hammering': 140, 'task_9_w_sawing': 87}, 'stir': {'task_1_k_cooking': 48, 'task_2_k_cooking_with_bowls': 49}, 'place': {'task_1_k_cooking': 96, 'task_2_k_cooking_with_bowls': 96, 'task_3_k_pouring': 96, 'task_4_k_wiping': 100, 'task_5_k_cereals': 201, 'task_6_w_hard_drive': 139, 'task_7_w_free_hard_drive': 140, 'task_8_w_hammering': 138, 'task_9_w_sawing': 88}, 'retreat': {'task_1_k_cooking': 192, 'task_2_k_cooking_with_bowls': 192, 'task_3_k_pouring': 106, 'task_4_k_wiping': 156, 'task_5_k_cereals': 380, 'task_6_w_hard_drive': 334, 'task_7_w_free_hard_drive': 232, 'task_8_w_hammering': 281, 'task_9_w_sawing': 126}, 'hold': {'task_1_k_cooking': 98, 'task_2_k_cooking_with_bowls': 96, 'task_3_k_pouring': 53, 'task_4_k_wiping': 113, 'task_5_k_cereals': 157, 'task_6_w_hard_drive': 106, 'task_7_w_free_hard_drive': 69, 'task_8_w_hammering': 187, 'task_9_w_sawing': 83}, 'pour': {'task_1_k_cooking': 48, 'task_2_k_cooking_with_bowls': 47, 'task_3_k_pouring': 48, 'task_5_k_cereals': 143}, 'drink': {'task_3_k_pouring': 48}, 'wipe': {'task_4_k_wiping': 48}, 'cut': {'task_5_k_cereals': 48}, 'screw': {'task_6_w_hard_drive': 137, 'task_7_w_free_hard_drive': 94}, 'hammer': {'task_8_w_hammering': 90}, 'saw': {'task_9_w_sawing': 46}}\n"
     ]
    }
   ],
   "source": [
    "print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_dict = {}\n",
    "\n",
    "for action, children in dict.items():\n",
    "    # Find the child node with the maximum count\n",
    "    max_child = max(children, key=children.get)\n",
    "    baseline_dict[action] = max_child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idle': 'task_8_w_hammering', 'approach': 'task_5_k_cereals', 'lift': 'task_5_k_cereals', 'stir': 'task_2_k_cooking_with_bowls', 'place': 'task_5_k_cereals', 'retreat': 'task_5_k_cereals', 'hold': 'task_8_w_hammering', 'pour': 'task_5_k_cereals', 'drink': 'task_3_k_pouring', 'wipe': 'task_4_k_wiping', 'cut': 'task_5_k_cereals', 'screw': 'task_6_w_hard_drive', 'hammer': 'task_8_w_hammering', 'saw': 'task_9_w_sawing'}\n"
     ]
    }
   ],
   "source": [
    "print(baseline_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Predictions by Baseline Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_utterances = []  # True values\n",
    "predictions = []  # Top 1 predictions\n",
    "predictions_probab = []  # All predictions - so this one will be the list of dictionaries, \n",
    "                         # with parent actions as keys, number of votes for that action as values.\n",
    "\n",
    "for node in test.nodes(data=True):\n",
    "    node_id = node[0]   # String, e.g. \"658190c06eccd77ab5dc84d4\"\n",
    "    node_data = node[1] # Dictionary with type, utterances, timestamp, etc.\n",
    "\n",
    "    if node_data['utterances'][0] in parent_actions: # If the node is a parent action node\n",
    "\n",
    "        real_utterance = node_data['utterances'][0] \n",
    "        predicted_utterances = [] \n",
    "\n",
    "        # Iterate through all the sub-actions and make predictions using baseline_dict.\n",
    "        for u, v, data in test.edges(node_id, data=True):\n",
    "            if data.get('link_type') == 'has_element':\n",
    "                sub_action = test.nodes[v].get('utterances')[0]\n",
    "                prediction = baseline_dict.get(sub_action)\n",
    "                predicted_utterances.append(prediction)\n",
    "                    \n",
    "        # Keep top 1 prediction\n",
    "        predicted_utterance = max(predicted_utterances,key=predicted_utterances.count) \n",
    "        predictions.append(predicted_utterance)\n",
    "        # Keep the real value\n",
    "        real_utterances.append(real_utterance)\n",
    "        # Keep all the predictions with corresponding votes\n",
    "        predictions_probab.append({i:predicted_utterances.count(i) for i in set(predicted_utterances)})\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_at_one(true, pred):\n",
    "    # true and pred are lists\n",
    "\n",
    "    n = len(true)\n",
    "    tp = sum([int(true[i]==pred[i]) for i in range (n)])\n",
    "    return tp/n\n",
    "\n",
    "def hits_at_k(true, pred, k):\n",
    "    # true is a list, pred is a list of dictionaries, \n",
    "    # e.g. {'task_5_k_cereals': 8, 'task_8_w_hammering': 4, 'task_9_w_sawing': 1}\n",
    "\n",
    "    n = len(true)\n",
    "    hits = np.zeros(n)\n",
    "    \n",
    "    for p in range (1, k+1):\n",
    "        # get key with p-th larest value from each dictionary in the pred list\n",
    "        pred_i = [sorted(pred[i], key=pred[i].get)[-p] if p <= len(pred[i]) else 'None' for i in range (n)]\n",
    "        for hit in range (n):\n",
    "            if hits[hit] == 0 and pred_i[hit] == true[hit]:\n",
    "                hits[hit] = 1\n",
    "        \n",
    "    tp = sum(hits)\n",
    "    return tp/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1111111111111111"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits_at_one(real_utterances, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits@1:  0.1111111111111111\n",
      "Hits@2:  0.23148148148148148\n",
      "Hits@3:  0.7685185185185185\n"
     ]
    }
   ],
   "source": [
    "for k in range (1, 4):\n",
    "    print(f'Hits@{k}: ', hits_at_k(real_utterances, predictions_probab, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
